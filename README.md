## ğŸ‘¥ Team: NITP | ğŸ” Project: VeracityStream

# ğŸ” Project VeracityStream: Problem Statement

## ğŸš¨ The Challenge (Problem Statement 3)
Generative AI models are widely used for research, learning, and decision-making. However, these models often generate **factually incorrect information** presented with high confidence.

A critical extension of this problem is the creation of **fake citations, non-existent references, and broken links**, which appear legitimate but cannot be verified. This makes it difficult for users to trust AI-generated content and may lead to:
* **Misinformation**
* **Legal Risks**
* **Ethical Concerns**

## ğŸ’¡ The Solution
Build a system that can **detect, flag, and verify** factual claims and citations generated by AI models, helping users distinguish between reliable and unreliable AI-produced information.

---

# ğŸ” VeracityStream  
> **Byte Quest: PS 03** | *Fighting AI Hallucinations with Real-Time Verification*

[![Flutter](https://img.shields.io/badge/Frontend-Flutter-02569B?logo=flutter&logoColor=white)](https://flutter.dev)
[![Python](https://img.shields.io/badge/Backend-Python_3.10+-3776AB?logo=python&logoColor=white)](https://www.python.org)
[![Status](https://img.shields.io/badge/Project_Status-Commit_2/8-brightgreen)](#)

---

## ğŸš© The Challenge: AIâ€™s â€œConfidence Gapâ€

Generative AI models are widely used for research, learning, and decision-making.  
However, they often generate **factually incorrect information with high confidence**, leading to trust, legal, and ethical risks.

**VeracityStream** is an automated verification layer designed to address:

- **Fabricated Facts** â€“ Claims without empirical evidence  
- **Ghost Citations** â€“ Fake, broken, or non-existent references  
- **Misinformation Risks** â€“ Harm caused by unreliable AI-generated content  



## ğŸ§  System Overview

<p align="center">
  <img src="assets/veracitystream-architecture.png"
       alt="VeracityStream System Architecture"
       width="700">
</p>

---

## ğŸ“‚ Project Structure
* `veracity_app/`: Flutter frontend.
* `veracity_backend/`: Python FastAPI engine & NLP logic.
---

## ğŸ“‚ Project Structure (Frontend)

Implemented in **Commit 2**, the Flutter application follows a **feature-first architecture with MVVM-inspired separation**, ensuring scalability and clean state management.

```text
lib/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ constants/       # App-wide strings and veracity labels
â”‚   â”œâ”€â”€ theme/           # Material 3 theme with custom veracity colors
â”‚   â””â”€â”€ network/         # HTTP/Dio client configuration
â”œâ”€â”€ features/
â”‚   â””â”€â”€ verification/    # Core feature: AI text analysis & UI
â”‚       â”œâ”€â”€ models/      # Claim and source data models
â”‚       â”œâ”€â”€ view/        # VeracityHighlighter and result widgets
â”‚       â””â”€â”€ viewmodel/   # API communication and verification state
â””â”€â”€ main.dart            # App entry point and theme initialization
```

**Flow:**

1. User inputs AI-generated text or a URL  
2. Text is decomposed into individual factual claims  
3. Claims and citations are verified using academic and web sources  
4. Results are scored and visualized in a color-coded UI  

---
## âœ¨ Core Features (Beta)
* **NLP Claim Decomposition**: Breaks long-form AI responses into individual factual units using spaCy.
* **Real-time Link Auditing**: Uses `requests` to ping generated citations and identify "Ghost Citations" (404s).
* **Confidence Mapping**: Differentiates between verified sources and hallucinated references.
---
## ğŸ§‘â€ğŸ’» Some Screeshots
<p align="centre">
  <img src="assets/image1.png"
       alt="Input text string"
       width="800">
</p>

<p align="centre">
  <img src="assets/image2.png"
       alt="Output"
       width="800">
</p>


## ğŸ¯ Key Objectives & Impact

### âœ… Automated Claim Extraction
Breaks unstructured AI text into independent, testable factual units using NLP.

### ğŸ”— Citation Audit Engine
- Validates URLs in real time  
- Detects fake or broken references  
- Verifies metadata against academic databases  

### ğŸ¨ Semantic Veracity Mapping
A Flutter-based UI that highlights:
- ğŸŸ¢ Verified claims  
- ğŸŸ¡ Uncertain claims  
- ğŸ”´ False or unverifiable claims  

---

## ğŸ› ï¸ Tech Stack

### Frontend
- Flutter (Mobile & Web)

### Backend
- Python 3.10+
- FastAPI
- LangGraph

### Verification APIs
- OpenAlex (Academic search)
- Crossref (Citation metadata)
- SerpAPI (Web verification)

---
## âš™ï¸ Setup & Installation

### Backend (Python/FastAPI)
1. Navigate to the backend directory: `cd veracity_backend`
2. Create a virtual environment: `python3 -m venv venv`
3. Activate the environment: `source venv/bin/activate`
4. Install dependencies: `pip install -r requirements.txt`
5. Run the server: `python main.py`

### Frontend (Flutter)
1. Navigate to the app directory: `cd veracity_app`
2. Get packages: `flutter pub get`
3. Run the app: `flutter run`
---

## ğŸ“Š Success Metrics

- **Detection Accuracy:**  
  > 90% identification of fabricated citations and invalid references

- **Verification Latency:**  
  Under 3 seconds per factual claim

- **Source Reliability:**  
  Preference for peer-reviewed and authoritative sources

---

## ğŸš€ Build Milestones

- [x] Commit 1: Initial Vision & Strategic Roadmap  
- [x] Commit 2: Flutter Project Scaffolding & Theme  
- [x] Commit 3: Backend â€“ NLP Claim Extraction  
- [x] Commit 4: Citation Metadata Validator  
- [ ] Commit 5: Real-Time Verification & Scoring API  
- [ ] Commit 6: Flutter â†” Backend API Integration  
- [ ] Commit 7: Flutter Veracity Highlight UI  
- [ ] Commit 8: Final Polish & Documentation  

---

## ğŸ“Œ Vision

> *â€œTrust is the missing layer in AI outputs. VeracityStream makes truth visible.â€*
